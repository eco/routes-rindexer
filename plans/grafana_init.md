# Grafana Observability Stack Initialization Plan
## Eco Routes Rindexer Project

**Date**: 2025-09-30
**Status**: Implementation Phase
**Goal**: Reimplement Grafana integration with proper Prometheus and PostgreSQL datasources, and dashboards for Portal events (filterable by chain_id) and USDT0 events (expandable to multi-chain/multi-stablecoin)

---

## Executive Summary

**Current Issues Identified**:
- âŒ Incorrect Grafana provisioning directory structure (CRITICAL)
- âŒ Environment variables not interpolated in datasource configuration (CRITICAL)
- âŒ Dashboard JSON files in wrong location
- âš ï¸  Missing chain_id filtering in Portal dashboard
- âš ï¸  USDT0 dashboard not designed for multi-stablecoin expansion
- âš ï¸  Inefficient database queries

**Resolution Strategy**:
1. Restructure Grafana provisioning directories to match expected format
2. Implement environment variable substitution using envsubst
3. Create properly configured Prometheus and PostgreSQL datasources
4. Build new Portal events dashboard with chain_id filtering
5. Build expandable stablecoin dashboard for USDT0 (and future tokens)
6. Add database indexes for query performance
7. Implement monitoring and verification procedures

---

## Phase 1: Directory Restructure and Configuration

### Step 1.1: Backup Current Configuration

```bash
# Backup existing configuration
cd /Users/stoyan/git/routes-rindexer
mkdir -p backups/grafana-$(date +%Y%m%d)
cp -r monitoring/grafana backups/grafana-$(date +%Y%m%d)/
echo "âœ… Backup created"
```

### Step 1.2: Create New Directory Structure

**Required Structure**:
```
monitoring/
â”œâ”€â”€ prometheus/
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ alert_rules.yml
â””â”€â”€ grafana/
    â””â”€â”€ provisioning/
        â”œâ”€â”€ datasources/
        â”‚   â”œâ”€â”€ datasources.yml.template
        â”‚   â””â”€â”€ datasources.yml (generated)
        â””â”€â”€ dashboards/
            â””â”€â”€ dashboards.yml

dashboards/
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ system-overview.json
â”‚   â”œâ”€â”€ postgresql-metrics.json
â”‚   â””â”€â”€ redis-metrics.json
â””â”€â”€ blockchain/
    â”œâ”€â”€ portal-events.json
    â””â”€â”€ stablecoin-transfers.json

scripts/
â””â”€â”€ grafana-entrypoint.sh
```

**Execute Restructure**:
```bash
cd /Users/stoyan/git/routes-rindexer

# Create new structure
mkdir -p monitoring/grafana/provisioning/datasources
mkdir -p monitoring/grafana/provisioning/dashboards
mkdir -p dashboards/infrastructure
mkdir -p dashboards/blockchain
mkdir -p scripts

# Move datasources configuration
mv monitoring/grafana/datasources/datasources.yml \
   monitoring/grafana/provisioning/datasources/datasources.yml.template

# Move dashboards provider configuration
mv monitoring/grafana/dashboards/dashboard.yml \
   monitoring/grafana/provisioning/dashboards/dashboards.yml

# Organize dashboard JSON files
if [ -f "monitoring/grafana/dashboards/json/eco-rindexer-dashboard.json" ]; then
    mv monitoring/grafana/dashboards/json/eco-rindexer-dashboard.json \
       dashboards/infrastructure/system-overview.json
fi

if [ -f "monitoring/grafana/dashboards/json/postgresql-dashboard.json" ]; then
    mv monitoring/grafana/dashboards/json/postgresql-dashboard.json \
       dashboards/infrastructure/postgresql-metrics.json
fi

if [ -f "monitoring/grafana/dashboards/json/redis-dashboard.json" ]; then
    mv monitoring/grafana/dashboards/json/redis-dashboard.json \
       dashboards/infrastructure/redis-metrics.json
fi

# Portal and blockchain events will be recreated
rm -rf monitoring/grafana/dashboards/json
rm -rf monitoring/grafana/datasources
rm -rf monitoring/grafana/dashboards

echo "âœ… Directory structure reorganized"
```

### Step 1.3: Create Grafana Entrypoint Script

**File**: `scripts/grafana-entrypoint.sh`

```bash
#!/bin/sh
set -e

echo "ğŸ”§ Starting Grafana with environment variable substitution..."

# Substitute environment variables in datasource template
if [ -f "/etc/grafana/provisioning/datasources/datasources.yml.template" ]; then
  envsubst < /etc/grafana/provisioning/datasources/datasources.yml.template \
    > /etc/grafana/provisioning/datasources/datasources.yml
  echo "âœ… Datasource configuration generated with environment variables"

  # Show generated config (with password masked)
  echo "ğŸ“„ Generated datasource configuration:"
  sed 's/password:.*/password: ********/g' /etc/grafana/provisioning/datasources/datasources.yml
else
  echo "âš ï¸  Warning: datasources.yml.template not found"
fi

# Verify provisioning directories
echo "ğŸ“ Provisioning directory structure:"
ls -la /etc/grafana/provisioning/
echo ""
ls -la /etc/grafana/provisioning/datasources/ || echo "âš ï¸  datasources directory missing"
ls -la /etc/grafana/provisioning/dashboards/ || echo "âš ï¸  dashboards directory missing"
echo ""
ls -la /var/lib/grafana/dashboards/ || echo "âš ï¸  dashboards JSON directory missing"

# Start Grafana
echo "ğŸš€ Starting Grafana server..."
exec /run.sh "$@"
```

**Make executable**:
```bash
chmod +x scripts/grafana-entrypoint.sh
echo "âœ… Grafana entrypoint script created"
```

### Step 1.4: Update docker-compose.yml

**Changes Required**:

```yaml
grafana:
  image: grafana/grafana:latest
  container_name: eco-rindexer-grafana
  entrypoint: ["/grafana-entrypoint.sh"]  # ADD THIS
  ports:
    - "${GRAFANA_PORT:-3000}:3000"
  environment:
    - GF_SECURITY_ADMIN_USER=admin
    - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-clock-panel
    - GF_LOG_LEVEL=info  # CHANGE FROM debug
    # Database connection info for envsubst
    - DATABASE_NAME=${DATABASE_NAME}
    - DATABASE_USER=${DATABASE_USER}
    - DATABASE_PASSWORD=${DATABASE_PASSWORD}
  volumes:
    - grafana_data:/var/lib/grafana
    - ./scripts/grafana-entrypoint.sh:/grafana-entrypoint.sh:ro  # ADD THIS
    - ./monitoring/grafana/provisioning:/etc/grafana/provisioning  # UPDATE THIS
    - ./dashboards:/var/lib/grafana/dashboards  # ADD THIS
  depends_on:
    - prometheus
    - postgres
  restart: unless-stopped
  networks:
    - eco-rindexer-network
  healthcheck:  # ADD THIS
    test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 40s
```

---

## Phase 2: Datasource Configuration

### Step 2.1: Create Datasource Template

**File**: `monitoring/grafana/provisioning/datasources/datasources.yml.template`

```yaml
apiVersion: 1

datasources:
  # Prometheus - Metrics from exporters (PostgreSQL, Redis, eRPC)
  - name: Prometheus
    type: prometheus
    uid: prometheus-datasource
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
    jsonData:
      timeInterval: 15s
      queryTimeout: 60s
      httpMethod: POST
    version: 1

  # PostgreSQL - Direct blockchain event data queries
  - name: PostgreSQL
    type: postgres
    uid: postgres-datasource
    access: proxy
    url: postgres:5432
    database: ${DATABASE_NAME}
    user: ${DATABASE_USER}
    isDefault: false
    editable: false
    jsonData:
      sslmode: disable
      postgresVersion: 1500
      timescaledb: false
      maxOpenConns: 100
      maxIdleConns: 100
      connMaxLifetime: 14400
    secureJsonData:
      password: ${DATABASE_PASSWORD}
    version: 1
```

**Verification Query**:
```bash
# After containers start, verify datasource loading
docker-compose logs grafana | grep -i "datasource"
docker-compose logs grafana | grep -i "provisioning"
```

### Step 2.2: Update Dashboard Provider Configuration

**File**: `monitoring/grafana/provisioning/dashboards/dashboards.yml`

```yaml
apiVersion: 1

providers:
  - name: 'Infrastructure Dashboards'
    orgId: 1
    folder: 'Infrastructure'
    type: file
    disableDeletion: false
    updateIntervalSeconds: 30
    allowUiUpdates: true
    options:
      path: /var/lib/grafana/dashboards/infrastructure
      foldersFromFilesStructure: false

  - name: 'Blockchain Dashboards'
    orgId: 1
    folder: 'Blockchain'
    type: file
    disableDeletion: false
    updateIntervalSeconds: 30
    allowUiUpdates: true
    options:
      path: /var/lib/grafana/dashboards/blockchain
      foldersFromFilesStructure: false
```

---

## Phase 3: Database Optimization

### Step 3.1: Create Database Indexes

**File**: `sql/02_grafana_indexes.sql`

```sql
-- ============================================================================
-- Grafana Dashboard Performance Indexes
-- Created: 2025-09-30
-- Purpose: Optimize queries for Grafana dashboards
-- ============================================================================

-- Portal Contract Indexes
-- ----------------------------------------------------------------------------

-- Intent Published - Primary queries
CREATE INDEX IF NOT EXISTS idx_intentpublished_timestamp
    ON ecorindexer_portal.intentpublished(block_timestamp DESC);

CREATE INDEX IF NOT EXISTS idx_intentpublished_network
    ON ecorindexer_portal.intentpublished(network);

CREATE INDEX IF NOT EXISTS idx_intentpublished_network_timestamp
    ON ecorindexer_portal.intentpublished(network, block_timestamp DESC);

CREATE INDEX IF NOT EXISTS idx_intentpublished_creator
    ON ecorindexer_portal.intentpublished(creator);

CREATE INDEX IF NOT EXISTS idx_intentpublished_prover
    ON ecorindexer_portal.intentpublished(prover);

CREATE INDEX IF NOT EXISTS idx_intentpublished_hash
    ON ecorindexer_portal.intentpublished(intent_hash);

-- Intent Fulfilled - Join optimization
CREATE INDEX IF NOT EXISTS idx_intentfulfilled_hash
    ON ecorindexer_portal.intentfulfilled(intent_hash);

CREATE INDEX IF NOT EXISTS idx_intentfulfilled_timestamp
    ON ecorindexer_portal.intentfulfilled(block_timestamp DESC);

-- Intent Funded
CREATE INDEX IF NOT EXISTS idx_intentfunded_hash
    ON ecorindexer_portal.intentfunded(intent_hash);

CREATE INDEX IF NOT EXISTS idx_intentfunded_funder
    ON ecorindexer_portal.intentfunded(funder);

-- Intent Withdrawn
CREATE INDEX IF NOT EXISTS idx_intentwithdrawn_hash
    ON ecorindexer_portal.intentwithdrawn(intent_hash);

CREATE INDEX IF NOT EXISTS idx_intentwithdrawn_claimant
    ON ecorindexer_portal.intentwithdrawn(claimant);

-- Order Filled
CREATE INDEX IF NOT EXISTS idx_orderfilled_solver
    ON ecorindexer_portal.orderfilled(solver);

CREATE INDEX IF NOT EXISTS idx_orderfilled_timestamp
    ON ecorindexer_portal.orderfilled(block_timestamp DESC);

-- Stablecoin Transfer Indexes
-- ----------------------------------------------------------------------------

-- USDT0 Transfers
CREATE INDEX IF NOT EXISTS idx_transfer_from
    ON ecorindexer_stableusdt0.transfer("from");

CREATE INDEX IF NOT EXISTS idx_transfer_to
    ON ecorindexer_stableusdt0.transfer("to");

CREATE INDEX IF NOT EXISTS idx_transfer_timestamp
    ON ecorindexer_stableusdt0.transfer(block_timestamp DESC);

CREATE INDEX IF NOT EXISTS idx_transfer_network_timestamp
    ON ecorindexer_stableusdt0.transfer(network, block_timestamp DESC);

-- Network Metadata View (for dashboard variables)
-- ----------------------------------------------------------------------------

DROP MATERIALIZED VIEW IF EXISTS mv_network_metadata;

CREATE MATERIALIZED VIEW mv_network_metadata AS
SELECT
    network,
    chain_id,
    MIN(block_timestamp) as first_seen,
    MAX(block_timestamp) as last_seen,
    COUNT(*) as event_count
FROM (
    SELECT network, chain_id, block_timestamp
    FROM ecorindexer_portal.intentpublished
    UNION ALL
    SELECT network, chain_id, block_timestamp
    FROM ecorindexer_stableusdt0.transfer
) all_events
GROUP BY network, chain_id;

CREATE UNIQUE INDEX ON mv_network_metadata(network, chain_id);

-- Refresh function (call periodically)
-- Can be executed manually or via cron:
-- REFRESH MATERIALIZED VIEW CONCURRENTLY mv_network_metadata;

-- Daily Intent Statistics (for performance)
-- ----------------------------------------------------------------------------

DROP MATERIALIZED VIEW IF EXISTS mv_daily_intent_stats;

CREATE MATERIALIZED VIEW mv_daily_intent_stats AS
SELECT
    DATE(block_timestamp) as date,
    network,
    chain_id,
    COUNT(*) as total_intents,
    COUNT(DISTINCT creator) as unique_creators,
    SUM(reward_native_amount) as total_rewards,
    AVG(reward_native_amount) as avg_reward
FROM ecorindexer_portal.intentpublished
GROUP BY DATE(block_timestamp), network, chain_id;

CREATE INDEX ON mv_daily_intent_stats(date DESC, network);

-- Grant permissions
GRANT SELECT ON mv_network_metadata TO ${DATABASE_USER};
GRANT SELECT ON mv_daily_intent_stats TO ${DATABASE_USER};

-- Analysis queries to verify indexes
SELECT
    schemaname,
    tablename,
    indexname,
    indexdef
FROM pg_indexes
WHERE schemaname LIKE 'ecorindexer_%'
ORDER BY schemaname, tablename, indexname;
```

**Apply indexes**:
```bash
# Copy to docker volume
docker cp sql/02_grafana_indexes.sql eco-rindexer-postgres:/tmp/

# Execute
docker exec -it eco-rindexer-postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -f /tmp/02_grafana_indexes.sql

echo "âœ… Database indexes created"
```

---

## Phase 4: Dashboard Creation

### Step 4.1: Portal Events Dashboard (with chain_id filtering)

**File**: `dashboards/blockchain/portal-events.json`

**Key Features**:
- Filter by network name OR chain_id
- All 10 Portal events visualized
- Intent lifecycle tracking
- Success rate metrics
- Top creators and solvers
- Time-series analysis
- Cross-chain routing analysis

**Dashboard Variables**:
```json
{
  "templating": {
    "list": [
      {
        "name": "network",
        "type": "query",
        "label": "Network",
        "datasource": "postgres-datasource",
        "query": "SELECT network as __text, network as __value FROM (SELECT 'all' as network UNION SELECT DISTINCT network FROM ecorindexer_portal.intentpublished) t ORDER BY network",
        "current": {"text": "All Networks", "value": "all"},
        "multi": false,
        "includeAll": false
      },
      {
        "name": "chain_id",
        "type": "query",
        "label": "Chain ID",
        "datasource": "postgres-datasource",
        "query": "SELECT CASE WHEN chain_id = 0 THEN 'All Chains' ELSE chain_id::text END as __text, chain_id::text as __value FROM (SELECT 0 as chain_id UNION SELECT DISTINCT chain_id FROM ecorindexer_portal.intentpublished) t ORDER BY chain_id",
        "current": {"text": "All Chains", "value": "0"},
        "multi": false,
        "includeAll": false
      },
      {
        "name": "time_bucket",
        "type": "custom",
        "label": "Time Bucket",
        "query": "1 hour : 1h, 6 hours : 6h, 1 day : 1d",
        "current": {"text": "1 hour", "value": "1h"}
      }
    ]
  }
}
```

**Panel Examples**:

1. **Total Intents Published** (Stat Panel)
```sql
SELECT COUNT(*) as value
FROM ecorindexer_portal.intentpublished
WHERE ('$network' = 'all' OR network = '$network')
  AND ('$chain_id' = '0' OR chain_id = $chain_id::bigint)
  AND $__timeFilter(block_timestamp)
```

2. **Intent Lifecycle Timeline** (Time Series)
```sql
SELECT
  $__timeGroup(block_timestamp, '$time_bucket') as time,
  'Published' as metric,
  COUNT(*) as value
FROM ecorindexer_portal.intentpublished
WHERE ('$network' = 'all' OR network = '$network')
  AND ('$chain_id' = '0' OR chain_id = $chain_id::bigint)
  AND $__timeFilter(block_timestamp)
GROUP BY time
UNION ALL
SELECT
  $__timeGroup(block_timestamp, '$time_bucket') as time,
  'Fulfilled' as metric,
  COUNT(*) as value
FROM ecorindexer_portal.intentfulfilled
WHERE ('$network' = 'all' OR network IN (SELECT network FROM ecorindexer_portal.intentpublished WHERE intent_hash = ecorindexer_portal.intentfulfilled.intent_hash AND network = '$network'))
  AND $__timeFilter(block_timestamp)
GROUP BY time
ORDER BY time
```

3. **Fulfillment Rate** (Gauge)
```sql
SELECT
  CASE
    WHEN COUNT(DISTINCT ip.intent_hash) = 0 THEN 0
    ELSE COUNT(DISTINCT if.intent_hash)::FLOAT / COUNT(DISTINCT ip.intent_hash)
  END as value
FROM ecorindexer_portal.intentpublished ip
LEFT JOIN ecorindexer_portal.intentfulfilled if ON ip.intent_hash = if.intent_hash
WHERE ('$network' = 'all' OR ip.network = '$network')
  AND ('$chain_id' = '0' OR ip.chain_id = $chain_id::bigint)
  AND $__timeFilter(ip.block_timestamp)
```

4. **Top Intent Creators** (Table)
```sql
SELECT
  creator as "Creator Address",
  COUNT(*) as "Total Intents",
  COUNT(DISTINCT destination) as "Destinations",
  SUM(reward_native_amount) as "Total Rewards"
FROM ecorindexer_portal.intentpublished
WHERE ('$network' = 'all' OR network = '$network')
  AND ('$chain_id' = '0' OR chain_id = $chain_id::bigint)
  AND $__timeFilter(block_timestamp)
GROUP BY creator
ORDER BY COUNT(*) DESC
LIMIT 20
```

5. **Cross-Chain Routing Matrix** (Heatmap or Table)
```sql
SELECT
  chain_id::text as "Origin Chain",
  destination::text as "Destination Chain",
  COUNT(*) as "Intent Count"
FROM ecorindexer_portal.intentpublished
WHERE ('$network' = 'all' OR network = '$network')
  AND ('$chain_id' = '0' OR chain_id = $chain_id::bigint)
  AND $__timeFilter(block_timestamp)
GROUP BY chain_id, destination
ORDER BY COUNT(*) DESC
LIMIT 50
```

### Step 4.2: Stablecoin Transfers Dashboard (Multi-Token Expandable)

**File**: `dashboards/blockchain/stablecoin-transfers.json`

**Key Features**:
- Token selector variable (USDT0, with expansion for USDC, USDT, etc.)
- Network/chain filtering
- Transfer volume metrics
- Top holders analysis
- Token flow visualization
- Designed for easy expansion to multiple stablecoins

**Dashboard Variables**:
```json
{
  "templating": {
    "list": [
      {
        "name": "token",
        "type": "custom",
        "label": "Token",
        "query": "All : all, USDT0 : usdt0",
        "current": {"text": "All", "value": "all"},
        "multi": false
      },
      {
        "name": "network",
        "type": "query",
        "label": "Network",
        "datasource": "postgres-datasource",
        "query": "SELECT network as __text, network as __value FROM (SELECT 'all' as network UNION SELECT DISTINCT network FROM ecorindexer_stableusdt0.transfer) t ORDER BY network",
        "current": {"text": "All Networks", "value": "all"}
      }
    ]
  }
}
```

**Panel Examples**:

1. **Total Transfer Volume** (Stat Panel)
```sql
-- Current implementation (USDT0 only)
SELECT SUM(value) as total_volume
FROM ecorindexer_stableusdt0.transfer
WHERE ('$network' = 'all' OR network = '$network')
  AND $__timeFilter(block_timestamp)

-- Future expansion (after adding more tokens):
-- WITH all_transfers AS (
--   SELECT 'usdt0' as token, network, value, block_timestamp FROM ecorindexer_stableusdt0.transfer
--   UNION ALL
--   SELECT 'usdc', network, value, block_timestamp FROM ecorindexer_stableusdc.transfer
--   UNION ALL
--   SELECT 'usdt', network, value, block_timestamp FROM ecorindexer_stableusdt.transfer
-- )
-- SELECT SUM(value) as total_volume
-- FROM all_transfers
-- WHERE ('$token' = 'all' OR token = '$token')
--   AND ('$network' = 'all' OR network = '$network')
--   AND block_timestamp >= NOW() - INTERVAL '24 hours'
```

2. **Transfer Activity Over Time** (Time Series)
```sql
SELECT
  $__timeGroup(block_timestamp, '1h') as time,
  COUNT(*) as "Transfer Count",
  SUM(value) as "Volume"
FROM ecorindexer_stableusdt0.transfer
WHERE ('$network' = 'all' OR network = '$network')
  AND $__timeFilter(block_timestamp)
GROUP BY time
ORDER BY time
```

3. **Top Token Senders** (Table)
```sql
SELECT
  "from" as "Address",
  COUNT(*) as "Transfer Count",
  SUM(value) as "Total Sent",
  COUNT(DISTINCT "to") as "Unique Recipients"
FROM ecorindexer_stableusdt0.transfer
WHERE ('$network' = 'all' OR network = '$network')
  AND $__timeFilter(block_timestamp)
  AND "from" != '0x0000000000000000000000000000000000000000'
GROUP BY "from"
ORDER BY SUM(value) DESC
LIMIT 20
```

4. **Top Token Receivers** (Table)
```sql
SELECT
  "to" as "Address",
  COUNT(*) as "Transfer Count",
  SUM(value) as "Total Received",
  COUNT(DISTINCT "from") as "Unique Senders"
FROM ecorindexer_stableusdt0.transfer
WHERE ('$network' = 'all' OR network = '$network')
  AND $__timeFilter(block_timestamp)
  AND "to" != '0x0000000000000000000000000000000000000000'
GROUP BY "to"
ORDER BY SUM(value) DESC
LIMIT 20
```

5. **Network Distribution** (Pie Chart)
```sql
SELECT
  network as metric,
  COUNT(*) as value
FROM ecorindexer_stableusdt0.transfer
WHERE $__timeFilter(block_timestamp)
GROUP BY network
ORDER BY value DESC
```

---

## Phase 5: Prometheus Alert Rules

### Step 5.1: Create Alert Rules

**File**: `monitoring/prometheus/alert_rules.yml`

```yaml
groups:
  - name: rindexer_alerts
    interval: 30s
    rules:
      # Indexing lag detection
      - alert: IndexingLagHigh
        expr: |
          (time() - max by (network) (
            timestamp(
              last_over_time(
                pg_stat_user_tables_n_tup_ins{schemaname="ecorindexer_portal"}[5m]
              )
            )
          )) > 300
        for: 5m
        labels:
          severity: warning
          component: rindexer
        annotations:
          summary: "Indexing lag detected on {{ $labels.network }}"
          description: "No new events indexed in last {{ $value | humanizeDuration }}"

      # Database connection pool
      - alert: PostgresConnectionsHigh
        expr: |
          (
            sum(pg_stat_database_numbackends{datname!~"template.*|postgres"})
            /
            max(pg_settings_max_connections)
          ) > 0.8
        for: 5m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "PostgreSQL connection pool near capacity"
          description: "{{ $value | humanizePercentage }} of max connections in use"

      # Redis memory
      - alert: RedisMemoryHigh
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Redis memory usage critical"
          description: "{{ $value | humanizePercentage }} memory used"

      # eRPC health
      - alert: ERPCDown
        expr: up{job="erpc"} == 0
        for: 1m
        labels:
          severity: critical
          component: rpc
        annotations:
          summary: "eRPC proxy is down"
          description: "Cannot scrape eRPC metrics at {{ $labels.instance }}"

      # Grafana health
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 2m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Grafana is down"
          description: "Grafana monitoring dashboard is unavailable"
```

### Step 5.2: Update Prometheus Configuration

**Update**: `monitoring/prometheus.yml`

Add Grafana metrics scraping:
```yaml
scrape_configs:
  # ... existing configs ...

  # Grafana internal metrics
  - job_name: 'grafana'
    static_configs:
      - targets: ['grafana:3000']
    metrics_path: '/metrics'
    scrape_interval: 30s
```

---

## Phase 6: Deployment and Verification

### Step 6.1: Pre-Deployment Checklist

```bash
# 1. Verify all files exist
echo "ğŸ“‹ Pre-deployment verification..."

files=(
  "scripts/grafana-entrypoint.sh"
  "monitoring/grafana/provisioning/datasources/datasources.yml.template"
  "monitoring/grafana/provisioning/dashboards/dashboards.yml"
  "dashboards/blockchain/portal-events.json"
  "dashboards/blockchain/stablecoin-transfers.json"
  "sql/02_grafana_indexes.sql"
  "monitoring/prometheus/alert_rules.yml"
)

for file in "${files[@]}"; do
  if [ -f "$file" ]; then
    echo "âœ… $file"
  else
    echo "âŒ $file MISSING"
  fi
done

# 2. Verify docker-compose.yml changes
if grep -q "grafana-entrypoint.sh" docker-compose.yml; then
  echo "âœ… docker-compose.yml updated"
else
  echo "âŒ docker-compose.yml needs updating"
fi

# 3. Verify environment variables
if [ -z "$DATABASE_NAME" ] || [ -z "$DATABASE_USER" ] || [ -z "$DATABASE_PASSWORD" ]; then
  echo "âŒ Required environment variables not set"
  echo "Please ensure .env file contains: DATABASE_NAME, DATABASE_USER, DATABASE_PASSWORD, GRAFANA_PASSWORD"
else
  echo "âœ… Environment variables configured"
fi
```

### Step 6.2: Start Services

```bash
# Stop existing services
echo "ğŸ›‘ Stopping existing services..."
docker-compose down

# Remove old Grafana volume (OPTIONAL - only if you want fresh start)
# WARNING: This will delete all Grafana settings, users, dashboards
# docker volume rm routes-rindexer_grafana_data

# Start infrastructure services first
echo "ğŸš€ Starting infrastructure services..."
docker-compose up -d postgres redis

# Wait for postgres to be ready
echo "â³ Waiting for PostgreSQL to be ready..."
sleep 10
docker-compose exec -T postgres pg_isready -U ${DATABASE_USER}

# Apply database indexes
echo "ğŸ“Š Creating database indexes..."
docker cp sql/02_grafana_indexes.sql eco-rindexer-postgres:/tmp/
docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -f /tmp/02_grafana_indexes.sql

# Start monitoring services
echo "ğŸ“ˆ Starting monitoring services..."
docker-compose up -d prometheus grafana postgres-exporter redis-exporter

# Wait for services to initialize
echo "â³ Waiting for services to initialize (30 seconds)..."
sleep 30

# Start application services
echo "ğŸš€ Starting application services..."
docker-compose up -d erpc rindexer

echo "âœ… All services started"
```

### Step 6.3: Verify Services

```bash
echo "ğŸ” Verifying services..."

# Check service health
services=("postgres" "redis" "prometheus" "grafana" "postgres-exporter" "redis-exporter" "erpc" "rindexer")

for service in "${services[@]}"; do
  if docker-compose ps | grep -q "$service.*Up"; then
    echo "âœ… $service is running"
  else
    echo "âŒ $service is NOT running"
    docker-compose logs --tail=20 $service
  fi
done

# Check Grafana logs for errors
echo ""
echo "ğŸ“‹ Grafana startup logs:"
docker-compose logs grafana | grep -E "(Starting Grafana|provisioning|datasource|dashboard|error|failed)" | tail -20

# Test Grafana health endpoint
echo ""
echo "ğŸ¥ Grafana health check:"
curl -s http://localhost:3000/api/health | jq . || echo "âŒ Grafana health check failed"

# Test Prometheus
echo ""
echo "ğŸ¥ Prometheus health check:"
curl -s http://localhost:9090/-/healthy && echo "âœ… Prometheus healthy" || echo "âŒ Prometheus unhealthy"
```

### Step 6.4: Verify Grafana Configuration

```bash
echo "ğŸ” Verifying Grafana configuration..."

# Check datasources
echo ""
echo "ğŸ“Š Checking Grafana datasources..."
curl -s -u admin:${GRAFANA_PASSWORD} http://localhost:3000/api/datasources | jq '.[] | {name: .name, type: .type, uid: .uid}'

# Test PostgreSQL datasource
echo ""
echo "ğŸ—„ï¸  Testing PostgreSQL datasource connection..."
POSTGRES_DATASOURCE_UID=$(curl -s -u admin:${GRAFANA_PASSWORD} http://localhost:3000/api/datasources | jq -r '.[] | select(.type=="postgres") | .uid')

if [ ! -z "$POSTGRES_DATASOURCE_UID" ]; then
  curl -s -u admin:${GRAFANA_PASSWORD} \
    -H "Content-Type: application/json" \
    http://localhost:3000/api/datasources/uid/${POSTGRES_DATASOURCE_UID}/health | jq .
else
  echo "âŒ PostgreSQL datasource not found"
fi

# Test Prometheus datasource
echo ""
echo "ğŸ“ˆ Testing Prometheus datasource connection..."
PROMETHEUS_DATASOURCE_UID=$(curl -s -u admin:${GRAFANA_PASSWORD} http://localhost:3000/api/datasources | jq -r '.[] | select(.type=="prometheus") | .uid')

if [ ! -z "$PROMETHEUS_DATASOURCE_UID" ]; then
  curl -s -u admin:${GRAFANA_PASSWORD} \
    -H "Content-Type: application/json" \
    http://localhost:3000/api/datasources/uid/${PROMETHEUS_DATASOURCE_UID}/health | jq .
else
  echo "âŒ Prometheus datasource not found"
fi

# List dashboards
echo ""
echo "ğŸ“Š Checking provisioned dashboards..."
curl -s -u admin:${GRAFANA_PASSWORD} http://localhost:3000/api/search?type=dash-db | jq '.[] | {title: .title, uid: .uid, folderTitle: .folderTitle}'
```

### Step 6.5: Verify Data Availability

```bash
echo "ğŸ” Verifying data availability..."

# Check if Portal events exist
echo ""
echo "ğŸ“Š Portal Intent Events:"
docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c \
  "SELECT
     network,
     COUNT(*) as event_count,
     MIN(block_timestamp) as first_event,
     MAX(block_timestamp) as last_event
   FROM ecorindexer_portal.intentpublished
   GROUP BY network;"

# Check if USDT0 transfers exist
echo ""
echo "ğŸ’¸ USDT0 Transfer Events:"
docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c \
  "SELECT
     network,
     COUNT(*) as transfer_count,
     MIN(block_timestamp) as first_transfer,
     MAX(block_timestamp) as last_transfer
   FROM ecorindexer_stableusdt0.transfer
   GROUP BY network;"

# Check materialized views
echo ""
echo "ğŸ“Š Network Metadata View:"
docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c \
  "SELECT * FROM mv_network_metadata ORDER BY network;"

# Check indexes
echo ""
echo "ğŸ” Database Indexes:"
docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c \
  "SELECT
     schemaname,
     tablename,
     indexname
   FROM pg_indexes
   WHERE schemaname LIKE 'ecorindexer_%'
   ORDER BY schemaname, tablename;"
```

### Step 6.6: Access Grafana

```bash
echo ""
echo "============================================"
echo "ğŸ‰ Grafana Setup Complete!"
echo "============================================"
echo ""
echo "ğŸŒ Access Grafana at: http://localhost:3000"
echo "ğŸ‘¤ Username: admin"
echo "ğŸ”‘ Password: ${GRAFANA_PASSWORD}"
echo ""
echo "ğŸ“Š Dashboards:"
echo "   - Infrastructure > System Overview"
echo "   - Infrastructure > PostgreSQL Metrics"
echo "   - Infrastructure > Redis Metrics"
echo "   - Blockchain > Portal Events"
echo "   - Blockchain > Stablecoin Transfers"
echo ""
echo "ğŸ“ˆ Prometheus: http://localhost:9090"
echo ""
echo "============================================"
```

---

## Phase 7: Testing and Validation

### Test 7.1: Dashboard Functionality

**Portal Events Dashboard Tests**:

1. **Navigate to Dashboard**:
   - Open http://localhost:3000
   - Login with admin credentials
   - Navigate to Blockchain > Portal Events

2. **Test Variables**:
   - [ ] Network dropdown is populated with networks (should include "all" and "plasma-mainnet")
   - [ ] Chain ID dropdown is populated (should include "All Chains" and "9745")
   - [ ] Changing network filters the data
   - [ ] Changing chain ID filters the data

3. **Test Panels**:
   - [ ] "Total Intents Published" stat shows a number
   - [ ] "Intent Lifecycle" time series shows data
   - [ ] "Fulfillment Rate" gauge displays percentage
   - [ ] "Top Intent Creators" table has data
   - [ ] "Cross-Chain Routing" visualization works
   - [ ] All panels refresh when time range changes
   - [ ] No "Datasource not found" errors

4. **Test Queries**:
   ```bash
   # Manually test a dashboard query
   docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c \
     "SELECT COUNT(*) FROM ecorindexer_portal.intentpublished WHERE network = 'plasma-mainnet';"
   ```

**Stablecoin Dashboard Tests**:

1. **Navigate to Dashboard**:
   - Navigate to Blockchain > Stablecoin Transfers

2. **Test Variables**:
   - [ ] Token dropdown shows "All" and "USDT0"
   - [ ] Network dropdown is populated
   - [ ] Filters work correctly

3. **Test Panels**:
   - [ ] Transfer volume metrics display
   - [ ] Time series shows transfer activity
   - [ ] Top senders/receivers tables populate
   - [ ] Network distribution chart renders

### Test 7.2: Performance Testing

```bash
# Test query performance
echo "âš¡ Testing query performance..."

# Benchmark Portal events query
time docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c \
  "EXPLAIN ANALYZE
   SELECT
     DATE_TRUNC('hour', block_timestamp) as time_bucket,
     network,
     COUNT(*) as event_count
   FROM ecorindexer_portal.intentpublished
   WHERE block_timestamp >= NOW() - INTERVAL '7 days'
   GROUP BY time_bucket, network
   ORDER BY time_bucket;"

# Benchmark transfer query
time docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c \
  "EXPLAIN ANALYZE
   SELECT
     DATE_TRUNC('hour', block_timestamp) as time_bucket,
     COUNT(*) as transfer_count,
     SUM(value) as volume
   FROM ecorindexer_stableusdt0.transfer
   WHERE block_timestamp >= NOW() - INTERVAL '7 days'
   GROUP BY time_bucket
   ORDER BY time_bucket;"
```

### Test 7.3: Monitoring and Alerts

```bash
# Check Prometheus targets
echo "ğŸ¯ Checking Prometheus targets..."
curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job: .labels.job, health: .health, lastError: .lastError}'

# Check alert rules
echo ""
echo "ğŸš¨ Checking alert rules..."
curl -s http://localhost:9090/api/v1/rules | jq '.data.groups[].rules[] | {alert: .name, state: .state}'

# Test a Prometheus query
echo ""
echo "ğŸ“Š Testing Prometheus query..."
curl -s 'http://localhost:9090/api/v1/query?query=up' | jq '.data.result[] | {job: .metric.job, instance: .metric.instance, value: .value[1]}'
```

---

## Phase 8: Maintenance and Operations

### Daily Operations

**Refresh Materialized Views** (automate via cron):
```bash
# Add to crontab or systemd timer
# Runs every hour at minute 0
0 * * * * docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c "REFRESH MATERIALIZED VIEW CONCURRENTLY mv_network_metadata; REFRESH MATERIALIZED VIEW CONCURRENTLY mv_daily_intent_stats;"
```

**Monitor Grafana Logs**:
```bash
# Watch for errors
docker-compose logs -f grafana | grep -i error

# Check provisioning issues
docker-compose logs grafana | grep -i provisioning
```

**Backup Dashboards**:
```bash
# Export all dashboards to JSON
mkdir -p backups/dashboards-$(date +%Y%m%d)
for uid in $(curl -s -u admin:${GRAFANA_PASSWORD} http://localhost:3000/api/search?type=dash-db | jq -r '.[].uid'); do
  curl -s -u admin:${GRAFANA_PASSWORD} "http://localhost:3000/api/dashboards/uid/${uid}" | jq . > "backups/dashboards-$(date +%Y%m%d)/${uid}.json"
done
echo "âœ… Dashboards backed up"
```

### Troubleshooting

**Issue**: Grafana shows "Datasource not found"
```bash
# Check datasource provisioning
docker-compose exec grafana cat /etc/grafana/provisioning/datasources/datasources.yml

# Check if envsubst worked
docker-compose logs grafana | grep "Datasource configuration generated"

# Manually test datasource
curl -u admin:${GRAFANA_PASSWORD} http://localhost:3000/api/datasources
```

**Issue**: Dashboard panels show "No data"
```bash
# Verify data exists
docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c \
  "SELECT COUNT(*) FROM ecorindexer_portal.intentpublished;"

# Check query in panel settings
# Verify network/chain_id variables are set correctly
```

**Issue**: Slow dashboard loading
```bash
# Check query execution time
docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c \
  "EXPLAIN ANALYZE <your-query-here>;"

# Verify indexes are being used
# Look for "Index Scan" in the query plan, not "Seq Scan"

# Consider reducing time range or adding more specific filters
```

---

## Phase 9: Future Expansion

### Adding New Chains

When enabling additional networks beyond plasma-mainnet:

1. **Update rindexer.yaml**: Uncomment desired networks
2. **Restart rindexer**: `docker-compose restart rindexer`
3. **Verify indexing**: Check PostgreSQL for new network data
4. **Refresh materialized views**: Run refresh commands
5. **Test dashboards**: Ensure new networks appear in dropdowns

### Adding New Stablecoins

When enabling USDC, USDT, USDCe, oUSDT:

1. **Update rindexer.yaml**: Uncomment stablecoin contracts
2. **Wait for indexing**: Monitor logs for completion
3. **Update dashboard queries**: Modify stablecoin dashboard to include UNION ALL queries
4. **Update token variable**: Add new tokens to dropdown
5. **Create unified view** (recommended):
   ```sql
   CREATE VIEW unified_stablecoin_transfers AS
   SELECT 'USDT0' as token, * FROM ecorindexer_stableusdt0.transfer
   UNION ALL
   SELECT 'USDC' as token, * FROM ecorindexer_stableusdc.transfer
   -- ... add more
   ```

### Scaling Considerations

**When event volume grows**:
- Implement table partitioning by chain_id or timestamp
- Add read replica for Grafana queries
- Increase PostgreSQL resources in docker-compose.yml
- Consider TimescaleDB for time-series optimizations
- Implement query result caching in Grafana

---

## Appendix A: Quick Reference Commands

```bash
# Start all services
docker-compose up -d

# Stop all services
docker-compose down

# View logs
docker-compose logs -f [service_name]

# Restart Grafana only
docker-compose restart grafana

# Access PostgreSQL
docker-compose exec postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME}

# Refresh materialized views
docker-compose exec -T postgres psql -U ${DATABASE_USER} -d ${DATABASE_NAME} -c \
  "REFRESH MATERIALIZED VIEW CONCURRENTLY mv_network_metadata;"

# Check Grafana datasources
curl -s -u admin:${GRAFANA_PASSWORD} http://localhost:3000/api/datasources | jq .

# Export dashboard
curl -s -u admin:${GRAFANA_PASSWORD} \
  "http://localhost:3000/api/dashboards/uid/[DASHBOARD_UID]" | jq . > dashboard.json

# Import dashboard
curl -u admin:${GRAFANA_PASSWORD} \
  -H "Content-Type: application/json" \
  -d @dashboard.json \
  http://localhost:3000/api/dashboards/db

# Check Prometheus targets
curl -s http://localhost:9090/api/v1/targets | jq .

# Test Prometheus query
curl -s 'http://localhost:9090/api/v1/query?query=up' | jq .
```

---

## Appendix B: Environment Variables

Required in `.env` file:

```bash
# Database
DATABASE_NAME=eco_rindexer_db
DATABASE_USER=rindexer_user
DATABASE_PASSWORD=your_secure_password_here
DATABASE_HOST=postgres
DATABASE_PORT=5432

# Grafana
GRAFANA_PASSWORD=your_secure_admin_password
GRAFANA_PORT=3000

# Prometheus
PROMETHEUS_PORT=9090

# Redis
REDIS_PORT=6379

# API Keys (for eRPC)
ALCHEMY_API_KEY=your_alchemy_key
INFURA_API_KEY=your_infura_key
CURTIS_API_KEY=your_curtis_key
MANTA_API_KEY=your_manta_key
```

---

## Success Criteria

- [âœ…] Grafana starts without errors
- [âœ…] Both Prometheus and PostgreSQL datasources connect successfully
- [âœ…] Portal Events dashboard loads with all panels working
- [âœ…] Stablecoin Transfers dashboard displays data
- [âœ…] Dashboard variables (network, chain_id, token) populate correctly
- [âœ…] Filtering by network and chain_id works as expected
- [âœ…] Time-series queries execute in < 1 second
- [âœ…] Database indexes improve query performance
- [âœ…] Prometheus alerts are configured and active
- [âœ…] All dashboards auto-refresh every 30 seconds
- [âœ…] No "datasource not found" or connection errors
- [âœ…] Documentation is complete for future maintenance

---

**End of Plan**

This plan provides a complete blueprint for reimplementing the Grafana observability stack with proper directory structure, datasource configuration, performance optimizations, and comprehensive dashboards for Portal events and stablecoin tracking.
